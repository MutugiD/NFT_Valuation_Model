{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import arange\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, HuberRegressor, LassoCV, Lasso, Ridge\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import List\n",
    "# display formatting for floats\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "token_id = [9093, 6485, 2578]\n",
    "collection = 'boredapeyachtclub.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv(collection, parse_dates=['SaleDate']).rename(columns={'Unnamed: 0': 'TokenId'})\n",
    "df = raw.loc[(raw['USDPrice'] != 0) & (~raw['LastSalePrice'].isna())]\n",
    "df = df.sort_values(['SaleDate']).reset_index(level=0, drop=True)\n",
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df.groupby('SaleDate').agg({'USDPrice': 'mean'})\n",
    "ts_outrm = ts.loc[ts['USDPrice'] < 10e5]\n",
    "log_price = np.log(ts_outrm['USDPrice'])\n",
    "rolling_median = log_price.rolling(window=14).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LogUSDPrice'] = np.log(df['USDPrice'])\n",
    "mean = df['LogUSDPrice'].ewm(span=14).mean()  # exponentially weighted moving average with 14 point window\n",
    "std = df['LogUSDPrice'].ewm(span=14).std()\n",
    "\n",
    "mean_plus_std = mean + 1.7*std  # 1.7 worked well\n",
    "# mean_minus_std = mean - 2*std\n",
    "\n",
    "# is_outlier = (df['LogUSDPrice'] > mean_plus_std) | (df['LogUSDPrice'] < mean_minus_std)\n",
    "is_outlier = df['LogUSDPrice'] > mean_plus_std\n",
    "df['Outlier'] = 1\n",
    "df.loc[is_outlier, 'Outlier'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo, dfi = df[is_outlier].copy(), df[~is_outlier].copy()\n",
    "dfs = dfo, dfi\n",
    "for data in dfs:\n",
    "    rolling_median = data['LogUSDPrice'].rolling(window=7, min_periods=1).median()\n",
    "    ewm = data['LogUSDPrice'].ewm(span=14).mean()\n",
    "    data['LogUSDPriceEWM'] = (rolling_median + ewm) / 2\n",
    "    # Percentage Extension from the Exponential Weighted Moving Average\n",
    "    data['PctExtensionEWM'] = data.apply(lambda x: (x['LogUSDPrice'] - x['LogUSDPriceEWM']) / x['LogUSDPriceEWM'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_periods(data): \n",
    "    dfs = dfo, dfi\n",
    "    for data in dfs:\n",
    "        rolling_median = data['LogUSDPrice'].rolling(window=7, min_periods=1).median()\n",
    "        ewm = data['LogUSDPrice'].ewm(span=14).mean()\n",
    "        data['LogUSDPriceEWM'] = (rolling_median + ewm) / 2\n",
    "        # Percentage Extension from the Exponential Weighted Moving Average\n",
    "        data['PctExtensionEWM'] = data.apply(lambda x: (x['LogUSDPrice'] - x['LogUSDPriceEWM']) / x['LogUSDPriceEWM'], axis=1)\n",
    "    return dfs\n",
    "\n",
    "def rarity_columns(data): \n",
    "    rarity_cols = [c for c in data.columns if 'Rarity' in c]  # get all numeric rarity related cols\n",
    "\n",
    "    def fill_trait_na(col): \n",
    "        fill_value = 1 - col.dropna().unique().sum()\n",
    "        return fill_value\n",
    "    for data in rolling_periods(data):\n",
    "        data[rarity_cols] = data[rarity_cols].apply(lambda x: x.fillna(fill_trait_na(x)))\n",
    "    return  rarity_cols\n",
    "#cols = rarity_processing(data)\n",
    "def traits(data): \n",
    "    traits = []\n",
    "    for cols in rarity_columns(data): \n",
    "        col = cols.split(\"Rarity\")\n",
    "        traits.append(col[0])\n",
    "    for data in dfs:\n",
    "        data[traits] = data[traits].fillna('None')\n",
    "    return traits \n",
    "rarity_cols = rarity_columns(data)\n",
    "traits = traits(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Eye ColorRarity',\n",
       " 'ClothingRarity',\n",
       " 'Eye Color',\n",
       " 'Clothing',\n",
       " 'CLSD GRN Breezy',\n",
       " 'BLCK-RD VARSITY JCKT')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def traits(data): \n",
    "    traits = []\n",
    "    for cols in rarity_columns(data): \n",
    "        col = cols.split(\"Rarity\")\n",
    "        traits.append(col[0])\n",
    "    for data in dfs:\n",
    "        data[traits] = data[traits].fillna('None')\n",
    "    return traits \n",
    "def top_rarity(df, n=2): \n",
    "    cols = [x for x in df.columns if 'Rarity' in x]\n",
    "    value = pd.DataFrame()\n",
    "    for col in cols:\n",
    "        value[col] = 1/df[col]\n",
    "    sum_total = value.loc['Total'] = value.sum()\n",
    "    top_two = sum_total.nlargest(n)\n",
    "    top_traits = top_two.index\n",
    "    return top_traits \n",
    "def rare_traits(df): \n",
    "    rarest_trait1 = df[top_rarity(df, n=2)[0].split(\"Rarity\")[0]]\n",
    "    rarest_trait2 = df[top_rarity(df, n=2)[1].split(\"Rarity\")[0]]\n",
    "    traits_df = pd.DataFrame({'rarest_trait1':rarest_trait1, 'rarest_trait2':rarest_trait2})\n",
    "\n",
    "    countnames1 = {}\n",
    "    for name in rarest_trait1:\n",
    "        if name in countnames1:\n",
    "            countnames1[name] += 1\n",
    "        else:\n",
    "            countnames1[name] = 1\n",
    "    rare_trait_1 = min(countnames1, key = countnames1.get)\n",
    "\n",
    "    countnames2 = {}\n",
    "    for name in rarest_trait2:\n",
    "        if name in countnames2:\n",
    "            countnames2[name] += 1\n",
    "        else:\n",
    "            countnames2[name] = 1\n",
    "    rare_trait_2 = min(countnames2, key = countnames2.get)\n",
    "    \n",
    "    return  rare_trait_1,  rare_trait_2 \n",
    "rarity_1 = top_rarity(df)[0]\n",
    "rarity_2 = top_rarity(df)[1]\n",
    "trait_1 = top_rarity(df)[0].split(\"Rarity\")[0]\n",
    "trait_2 = top_rarity(df)[1].split(\"Rarity\")[0]\n",
    "rare_trait_1 = rare_traits(df)[0]\n",
    "rare_trait_2 = rare_traits(df)[1]\n",
    "rarity_1, rarity_2, trait_1, trait_2, rare_trait_1, rare_trait_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_two_less_1pct(row, cols):\n",
    "    \"\"\"Retruns true if the NFT has at least two traits with rarities less than 1%\"\"\"\n",
    "    rarity = row[cols].values\n",
    "    n = len(rarity[np.where(rarity < 0.01)])\n",
    "    if n> 1:\n",
    "        return 1\n",
    "    return 0\n",
    "def rarity_1_or_rarity_2(row):\n",
    "    \"\"\"Return true if the NFT has clothes or fur with a rarity of less than 1%\"\"\"\n",
    "    if row[rarity_1] < 0.01 or row[rarity_2] < 0.01:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def trait_1_or_trait_2(row):\n",
    "    \"\"\"Return true if the NFT has Black Suit for clothes or Solid Gold fur\"\"\"\n",
    "    if row[trait_1] == rare_trait_1  or  row[trait_2] == rare_trait_2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def find_matches(row, categories):\n",
    "    traits = row[categories].values\n",
    "    keywords = []\n",
    "    for trait in traits:\n",
    "        split = trait.split(' ')\n",
    "        for word in split:\n",
    "            keywords.append(word)\n",
    "    if 'None' in keywords:\n",
    "        keywords.remove('None')\n",
    "    counts = Counter(keywords)\n",
    "    most_common = counts.most_common(1)\n",
    "    matches = most_common[0][1] - 1\n",
    "    if matches:\n",
    "        return 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = rolling_periods(df)\n",
    "rarity_cols = rarity_columns(df)\n",
    "traits = traits(df)\n",
    "for data in dfs:\n",
    "        data['HasTwoLess1Pct'] = data.apply(lambda x: has_two_less_1pct(x, rarity_cols), axis=1)\n",
    "        data['Rarity1OrRarity1'] = data.apply(rarity_1_or_rarity_2, axis=1)\n",
    "        data['Trait1OrTrait2'] = data.apply(trait_1_or_trait_2, axis=1)\n",
    "        data['HasMatches'] = data.apply(lambda x: find_matches(x, traits), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in dfs:\n",
    "    data.loc[(data['HasTwoLess1Pct']==1) | (data['Rarity1OrRarity1']==1) | (data['Trait1OrTrait2']==1), 'OutlierRule'] = -1\n",
    "    data['OutlierRule'] = data['OutlierRule'].fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_encoding(dfo): \n",
    "    categorical_feature_cols = traits \n",
    "    clf_features = ['HasTwoLess1Pct','Rarity1OrRarity1', 'Trait1OrTrait2', 'HasMatches']\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(dfo[categorical_feature_cols]).toarray())\n",
    "    enc_df.index = list(enc_df.index)  # convert index to int64 index\n",
    "    enc_df.columns = enc.get_feature_names_out()\n",
    "    dfp = dfo.reset_index(drop=True).merge(enc_df.reset_index(drop=True), left_index=True, right_index=True)\n",
    "    dfo = dfp.drop(columns=categorical_feature_cols)\n",
    "    return dfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inlier_encoding(dfi): \n",
    "    categorical_feature_cols = traits \n",
    "    clf_features = ['HasTwoLess1Pct','Rarity1OrRarity1', 'Trait1OrTrait2', 'HasMatches']\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(dfi[categorical_feature_cols]).toarray())\n",
    "    enc_df.columns = enc.get_feature_names_out()\n",
    "    dfp = dfi.reset_index(drop=True).merge(enc_df.reset_index(drop=True), left_index=True, right_index=True)\n",
    "    dfi = dfp.drop(columns=categorical_feature_cols)\n",
    "    return dfi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo = outlier_encoding(dfo)\n",
    "dfi = inlier_encoding(dfi)\n",
    "df = pd.concat([dfi, dfo])\n",
    "df.fillna(0,inplace=True)\n",
    "df1 = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_train_test(df, token_id):  \n",
    "    features = [feature for feature in df.columns if '_' in feature]\n",
    "    features.extend(['HasMatches', 'NumberOfSales'])\n",
    "    token = df.loc[df['TokenId'].isin(token_id)]\n",
    "    df = df[df['TokenId'].isin(token_id) == False]\n",
    "    X = df.loc[:, features+['OutlierRule', 'Outlier']]\n",
    "    y = df['PctExtensionEWM']\n",
    "    x_token = token.loc[:, features+['OutlierRule', 'Outlier']]\n",
    "    y_token = token['PctExtensionEWM']\n",
    "    return X, y, x_token, y_token \n",
    "\n",
    "X, y, x_token, y_token  = token_train_test(df, token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X, y):  \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X_train_in = X_train[X_train['Outlier']==1].drop(columns=['OutlierRule', 'Outlier'])\n",
    "    y_train_in = y_train[X_train['Outlier']==1]\n",
    "    X_train_out = X_train[X_train['Outlier']==-1].drop(columns=['OutlierRule', 'Outlier'])\n",
    "    y_train_out = y_train[X_train['Outlier']==-1]\n",
    "    return X_train, X_test, y_train, y_test, X_train_in, y_train_in,  X_train_out,  y_train_out\n",
    "X_train, X_test, y_train, y_test, X_train_in, y_train_in,  X_train_out,  y_train_out = train_test(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(models: List[dict], scores: List[str], X_train, y_train) -> dict:\n",
    "    \"\"\"Find the best model\"\"\"\n",
    "    results = {}\n",
    "    for candidate in models:\n",
    "        model = GridSearchCV(\n",
    "            candidate['constructor'], param_grid=candidate['param_grid'], scoring=scores\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        mean_test_score = model.cv_results_['mean_test_score'].mean()\n",
    "        results[candidate['name']] = mean_test_score\n",
    "    print(\"The winner is: {}\".format(\n",
    "        max(results, key=results.get)\n",
    "    ))\n",
    "    return results\n",
    "models = [\n",
    "    {\n",
    "        'name': 'Lasso',\n",
    "        'constructor': Lasso(),\n",
    "        'param_grid': {'alpha': [0.2, 0.4, 0.6, 0.8, 1.0]}\n",
    "    },\n",
    "    {\n",
    "        'name': 'RandomForest',\n",
    "        'constructor': RandomForestRegressor(random_state=0),\n",
    "        'param_grid': {}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Huber',\n",
    "        'constructor': HuberRegressor(),\n",
    "        'param_grid': {'epsilon': [10], 'max_iter': [10000]}\n",
    "    }\n",
    "]\n",
    "model_selection(models, 'neg_root_mean_squared_error', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import metrics\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('randomforest', RandomForestRegressor(random_state=42, n_estimators=200)),\n",
    "              ('linear', LassoCV(random_state=42, max_iter=10000)),\n",
    "              ('huber', HuberRegressor(epsilon=2, max_iter=10000))]\n",
    "vreg_in, vreg_out = VotingRegressor(estimators),  VotingRegressor(estimators)\n",
    "cross_validate(vreg_in, X_train_in, y_train_in, cv=3,\n",
    "               scoring=('neg_mean_squared_error'),\n",
    "               return_train_score=True)\n",
    "vreg_in.fit(X_train_in, y_train_in)\n",
    "vreg_out.fit(X_train_out, y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {'o': vreg_out, 'i': vreg_in}\n",
    "import pickle\n",
    "pickle_out = open(\"model.pkl\",\"wb\")\n",
    "pickle.dump(model, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, models: dict):\n",
    "    \"\"\"Custom prediction function to combine two regression models\"\"\"\n",
    "    outliers = X['OutlierRule'] == -1\n",
    "    X_o = X.loc[outliers].drop(columns=['OutlierRule', 'Outlier'])\n",
    "    X_i = X.loc[~outliers].drop(columns=['OutlierRule', 'Outlier'])\n",
    "    y = np.empty(X.shape[0])  # store combined predictions\n",
    "    # predict target values separately accorindg to the two models\n",
    "    \n",
    "    if X_o.shape[0]:\n",
    "        y_o_pred = models['o'].predict(X_o)\n",
    "        y[outliers] = y_o_pred\n",
    "    if X_i.shape[0]:\n",
    "        y_i_pred = models['i'].predict(X_i)\n",
    "        y[~outliers] = y_i_pred\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mapping(y_test): \n",
    "    y_actual, y_pred = y_test, predict(x_token, {'o': vreg_out, 'i': vreg_in})\n",
    "    predict_df = pd.DataFrame({'Actual': y_actual, 'Predicted': y_pred})\n",
    "    predicted_merged = df.merge(predict_df, how='left', left_index=True, right_index=True)\n",
    "    predicted_merged = predicted_merged.loc[~predicted_merged['Predicted'].isna()]\n",
    "    cols = predicted_merged.columns[0:]\n",
    "    pm = predicted_merged.loc[:, ['TokenId', 'SaleDate', 'LogUSDPriceEWM', 'Actual', 'Predicted', 'USDPrice', 'PctExtensionEWM']]\n",
    "    pm['PredictedUSDPrice'] = np.exp(pm['LogUSDPriceEWM'] * (1 + pm['Predicted']))\n",
    "    pm['Accuracy'] = 1 - abs(pm['PredictedUSDPrice'] - pm['USDPrice']) / ((pm['PredictedUSDPrice'] + pm['USDPrice']) / 2)\n",
    "    return pm[['TokenId', 'USDPrice', 'PredictedUSDPrice', 'Accuracy']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mapping(y_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
